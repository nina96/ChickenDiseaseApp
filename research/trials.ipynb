{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "# pprevent annoying tensorflow warning\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "import warnings\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', 90)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir=r'../input/chicken-disease-1/Train'\n",
    "csvpath=r'../input/chicken-disease-1/train_data.csv'\n",
    "df=pd.read_csv(csvpath)\n",
    "df.columns=['filepaths', 'labels' ]\n",
    "df['filepaths']=df['filepaths'].apply(lambda x: os.path.join(sdir,x))\n",
    "print(df.head())\n",
    "\n",
    "#below we are calculating data split ratio training split is 90%, valid split is 5% and 10% test data \n",
    "trsplit=.9 \n",
    "vsplit=.05\n",
    "dsplit =vsplit/(1-trsplit)\n",
    "#Using stratify is especially important when dealing with classification tasks, where the goal is to predict class labels. By using stratified sampling, you can improve the generalization of your model and make it more robust to handle different class distributions in real-world scenarios.\n",
    "strat=df['labels']\n",
    "train_df, dummy_df=train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify=strat)\n",
    "strat=dummy_df['labels']\n",
    "test_df, valid_df=train_test_split(dummy_df, train_size=dsplit, shuffle=True, random_state=123, stratify=strat)\n",
    "print('train_df lenght: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\n",
    "classes=list(train_df['labels'].unique())\n",
    "class_count = len(classes)\n",
    "groups=df.groupby('labels')\n",
    "print('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\n",
    "for label in train_df['labels'].unique():\n",
    "      group=groups.get_group(label)      \n",
    "      print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim (df, max_size, min_size, column):\n",
    "    df=df.copy()\n",
    "    original_class_count= len(list(df[column].unique()))\n",
    "    print ('Original Number of classes in dataframe: ', original_class_count)\n",
    "    sample_list=[] \n",
    "    groups=df.groupby(column)\n",
    "    for label in df[column].unique():        \n",
    "        group=groups.get_group(label)\n",
    "        sample_count=len(group)         \n",
    "        if sample_count> max_size :\n",
    "            strat=group[column]\n",
    "            samples,_=train_test_split(group, train_size=max_size, shuffle=True, random_state=123, stratify=strat)            \n",
    "            sample_list.append(samples)\n",
    "        elif sample_count>= min_size:\n",
    "            sample_list.append(group)\n",
    "    df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n",
    "    final_class_count= len(list(df[column].unique())) \n",
    "    if final_class_count != original_class_count:\n",
    "        print ('*** WARNING***  dataframe has a reduced number of classes' )\n",
    "    balance=list(df[column].value_counts())\n",
    "    print (balance)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples=500\n",
    "min_samples=0\n",
    "column = 'labels'\n",
    "train_df=trim(train_df, max_samples, min_samples, column)\n",
    "img_size=(224,224)\n",
    "working_dir=r'./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20 # We will use and EfficientetB5 model, with image size of (224, 224) this size should not cause resource error\n",
    "trgen=ImageDataGenerator(horizontal_flip=True,rotation_range=20, width_shift_range=.2,\n",
    "                                  height_shift_range=.2, zoom_range=.2 )\n",
    "t_and_v_gen=ImageDataGenerator()\n",
    "train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
    "# for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n",
    "# this insures that we go through all the sample in the test set exactly once.\n",
    "length=len(test_df)\n",
    "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
    "test_steps=int(length/test_batch_size)\n",
    "test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "# from the generator we can get information we will need later\n",
    "classes=list(train_gen.class_indices.keys())\n",
    "class_indices=list(train_gen.class_indices.values())\n",
    "class_count=len(classes)\n",
    "labels=test_gen.labels\n",
    "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)\n",
    "print ('{0:^25s}{1:^12s}'.format('class name', 'class index'))\n",
    "for klass, index in zip(classes, class_indices):\n",
    "    print(f'{klass:^25s}{str(index):^12s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_samples(gen ):\n",
    "    t_dict=gen.class_indices\n",
    "    classes=list(t_dict.keys())    \n",
    "    images,labels=next(gen) # get a sample batch from the generator \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    length=len(labels)\n",
    "    if length<25:   #show maximum of 25 images\n",
    "        r=length\n",
    "    else:\n",
    "        r=25\n",
    "    for i in range(r):        \n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image=images[i] /255       \n",
    "        plt.imshow(image)\n",
    "        index=np.argmax(labels[i])\n",
    "        class_name=classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "show_image_samples(train_gen )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape=(img_size[0], img_size[1], 3)\n",
    "model_name='EfficientNetB7'\n",
    "base_model=tf.keras.applications.efficientnet.EfficientNetB7(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n",
    "# Note you are always told NOT to make the base model trainable initially- that is WRONG you get better results leaving it trainable\n",
    "base_model.trainable=True\n",
    "x=base_model.output\n",
    "x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "x = Dense(1024, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.3, seed=123)(x)\n",
    "x = Dense(128, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.45, seed=123)(x)        \n",
    "output=Dense(class_count, activation='softmax')(x)\n",
    "model=Model(inputs=base_model.input, outputs=output)\n",
    "lr=.001 # start with this learning rate\n",
    "model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASK(keras.callbacks.Callback):\n",
    "    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n",
    "        super(ASK, self).__init__()\n",
    "        self.model=model               \n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask=True # if True query the user on a specified epoch\n",
    "        \n",
    "    def on_train_begin(self, logs=None): # this runs on the beginning of training\n",
    "        if self.ask_epoch == 0: \n",
    "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
    "            self.ask_epoch=1\n",
    "        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n",
    "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
    "            self.ask=False # do not query the user\n",
    "        if self.epochs == 1:\n",
    "            self.ask=False # running only for 1 epoch so do not query user\n",
    "        else:\n",
    "            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n",
    "            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n",
    "        self.start_time= time.time() # set the time at which training started\n",
    "        \n",
    "    def on_train_end(self, logs=None):   # runs at the end of training     \n",
    "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print (msg, flush=True) # print out training duration time\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        if self.ask: # are the conditions right to query the user?\n",
    "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n",
    "                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n",
    "                ans=input()\n",
    "                \n",
    "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
    "                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n",
    "                    self.model.stop_training = True # halt training\n",
    "                else: # user wants to continue training\n",
    "                    self.ask_epoch += int(ans)\n",
    "                    if self.ask_epoch > self.epochs:\n",
    "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
    "                    else:\n",
    "                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=40\n",
    "ask_epoch=10\n",
    "ask=ASK(model, epochs,  ask_epoch)\n",
    "rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2,verbose=1)\n",
    "estop=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, verbose=1,restore_best_weights=True)\n",
    "callbacks=[rlronp, estop, ask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data,start_epoch):\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout    \n",
    "    plt.show()\n",
    "    \n",
    "tr_plot(history,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= []\n",
    "y_true=test_gen.labels\n",
    "errors=0\n",
    "preds=model.predict(test_gen, steps=test_steps, verbose=1) # predict on the test set\n",
    "tests=len(preds)\n",
    "for i, p in enumerate(preds):\n",
    "        pred_index=np.argmax(p)         \n",
    "        true_index=test_gen.labels[i]  # labels are integer values\n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors=errors + 1\n",
    "        y_pred.append(pred_index)\n",
    "acc=( 1-errors/tests) * 100\n",
    "print(f'there were {errors} in {tests} tests for an accuracy of {acc:6.2f}')\n",
    "ypred=np.array(y_pred)\n",
    "ytrue=np.array(y_true)\n",
    "cm = confusion_matrix(ytrue, ypred )\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "clr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='chickens' \n",
    "acc=str(( 1-errors/tests) * 100)\n",
    "index=acc.rfind('.')\n",
    "acc=acc[:index + 3]\n",
    "save_id= subject + '_' + str(acc) + '.h5' \n",
    "model_save_loc=os.path.join(working_dir, save_id)\n",
    "model.save(model_save_loc)\n",
    "print ('model was saved as ' , model_save_loc ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
